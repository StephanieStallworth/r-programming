---
title: "Longley Linear Regression"
author: "Stephanie Stallworth"
date: "April 21, 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, comment ="")
```

###**Executive Summary**

Linear regression was developed in the field of statistics and is studied as a model for understanding the relationship between input and output numeric variables.  More specifically, it is a linear model that assumes a linear relationship between the input variables (x) and the single output variable (y).  It assumes that y can be calcuated from a linear combination of the input variables (x). As such, both the input values (x) and the output value (y) are numeric. 

In addition to being a statisical algorithm, linear regression is also used in machine learning for making predictions.  There are a number of techniques that could be used to prepare (or train) the linear regression equation from the data, four of which I demonstrate in this analysis:

1. Ordinary Least Squares Regression (OLS)    
2. Stepwize Linear Regression   
3. Principal Component Regression (PCR)  
4. Partial Least Squares Regression (PLS)  


###**PreProcessing**
I'll be applying these techniques to the `longley` dataset, which describes 7 economic variables observed from 1947 to 1962 used to predict the number of people employed yearly. 

```{r}

# Load statistical packages
library(pls)

# Load data
data(longley)
```
```{r}
# Examine structure and variable types
str(longley)

# View first few lines
head(longley)

# Summarize the data set
summary(longley)

# Visualize the data set 
x<-longley[ ,1:7]

par(mfrow=c(1,7))
  for(i in 1:7) {
  boxplot(x[,i], main=names(longley)[i])
  }

```

### **Linear Regression 1: Ordinary Least Squares Regression**
When there is more than one input variable, Ordinary Least Squares regression (OLS) can be used to estimate the values of the coefficients.  This regression is a linear model that seeks to find a set of coefficients for a line/hyper-plane that minimizes the sum of the squared errors. In other words, given a regression line through the data, it calculates the distance from each data point to the regression line, squares it, and sums all of the squared errors together. *This* is the quantity that ordinary least squares seeks to minimize.


```{r}
# Fit model
olsFit<-lm(Employed ~.,longley)

# Summarize the fit
summary(olsFit)

# Make predictions
olsPredictions<-predict(olsFit, longley)

# Summarize accuracy
olsMSE<-mean((longley$Employed - olsPredictions)^2)
print(olsMSE)
```

###**Linear Regression 2: Stepwize Linear Regression**
Stepwize Linear Regression is a method that makes use of linear regression to discover which subset of attributes in the dataset result in the best performing model. It is step-wise because each iteration of the method makes a change to the set of attributes and creates a model to evaluate the performance of the set.

```{r}
# Fit model
base<-lm(Employed ~., longley)

# Summarize the fit
summary(base)

# Perform step-wise feature selection
slrFit<-step(base)

# Summarize selected model
summary(slrFit)

# Make predictions
slrPredictions<-predict(slrFit, longley)

# Summarize accuracy
slrMSE<-mean((longley$Employed - slrPredictions)^2)
print(slrMSE)

```

###**Linear Regression 3: Principal Component Regression**
Principal Component Regression (PCR) creates a linear regression model using the outputs of a Principal Component Analysis (PCA) to estimate the coefficients of the model. PCR is useful when the data has highly correlated predictors.


```{r}
# Fit model
pcrFit<-pcr(Employed ~ ., data = longley, valdiation = "cv")

# Summarize the fit
summary(pcrFit)

# Make predictions
pcrPredictions<-predict(pcrFit, longley, ncomp = 6)

# Summarize accuracy
pcrMSE<-mean((longley$Employed - pcrPredictions)^2)
print(pcrMSE)
```

###**Linear Regression 4: Partial Least Squares Regression**
Partial Least Squares (PLS) Regression creates a linear model of the data in a transformed projection of problem space. Like PCR, PLS is appropriate for data with highly-correlated predictors.

```{r}
# Fit model
plsFit<-plsr(Employed ~., data = longley, validation = "CV")

# Summarize the fit
summary(plsFit)

# Make predictions
plsPredictions<-predict(plsFit, longley, ncomp = 6)

#Summarize acuracy
plsMSE<-mean((longley$Employed - plsPredictions)^2)
print(plsMSE)
```

